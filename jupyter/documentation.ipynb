{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pViAbjuYASA9"
   },
   "source": [
    "# ITS Support Chatbot\n",
    "\n",
    "This chatbot is an educational tool that's built to answer questions related to the CSUSB's [Information Technology Services](https://www.csusb.edu/its). The chatbot was built by team 1 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the ITS website as the primary data source.\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1)\n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1/wiki)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#1-Setup)  \n",
    "    - 1.1. [Requirements and Environment](#1.1-Requirements-and-Environment)  \n",
    "    - 1.2. [Import Required Libraries](#1.2-Importing-Required-Libraries)  \n",
    "    - 1.3. [Set Up Environment Variables](#1.3-Set-Up-Environment-Variables)  \n",
    "2. [Building the Chatbot](#2.-Building-the-Chatbot)  \n",
    "    - 2.1 [Vector Store and Embeddings](#2.1-Vector-Store-and-Embeddings)\n",
    "        - 2.1.1. [Create Vector Store](#2.1.1-Function-to-fetch-the-embedding-model)  \n",
    "        - 2.1.2. [Fetch Embedding Model](#2.1.2-Function-to-fetch-the-embedding-model)  \n",
    "    - 2.2. [Document Handling](#2.-Document-Handling)  \n",
    "        - 2.2.1. [Text Cleaning](#2.1-Function-to-Clean-Text)  \n",
    "        - 2.2.2. [Clean HTML Content](#2.2-Function-to-Clean-and-Extract-Text-from-HTML-Content)  \n",
    "        - 2.2.3. [Load Documents from the Web](#2.3-Function-for-loading-documents-from-the-web)\n",
    "    - 2.3 [Milvus Vector Store Management](#2.3.-Milvus-Vector-Store-Management)\n",
    "        - 2.3.1. [Load Existing Vector Store](#2.3.1-Function-to-load-existing-vector-store-(Milvus-database))\n",
    "        - 2.3.2. [Split Documents into Chunks](#2.3.2-Function-to-split-documents)   \n",
    "        - 2.3.3. [Create New Vector Store](#2.3.3-Function-to-Create-New-Vector-Store-(Milvus-database))  \n",
    "        - 2.3.4. [Initialize Milvus](#2.3.4-Core-function-for-initializing-Milvus)\n",
    "        - 2.3.5. [Initializing Vector Store](#2.3.5-Initializing-Vector-Store)\n",
    "3. [Testing the Chatbot](#3.-Testing-the-Chatbot)  \n",
    "    - 3.1. [Create RAG Prompt](#3.1-Function-to-create-RAG-prompt)  \n",
    "    - 3.2. [Query RAG](#3.2-Function-to-query-RAG-model)  \n",
    "    - 3.3. [Retrieve RAG Response](#3.3-Get-response-from-RAG)  \n",
    "4. [Conclusion](#4-Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpLCQScOASA-"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTiY7XcqASA_"
   },
   "source": [
    "Environment Installation:\n",
    "- Install ipykernel and virtualenv\n",
    "- Activate new virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HwcflKxFASA_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipykernel\n",
      "  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Using cached debugpy-1.8.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Using cached ipython-8.30.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
      "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
      "  Using cached jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting nest-asyncio (from ipykernel)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting packaging (from ipykernel)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting psutil (from ipykernel)\n",
      "  Using cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting pyzmq>=24 (from ipykernel)\n",
      "  Using cached pyzmq-26.2.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting tornado>=6.1 (from ipykernel)\n",
      "  Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel)\n",
      "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack_data (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from jupyter-client>=6.1.12->ipykernel)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
      "  Using cached platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Using cached parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
      "Using cached comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Using cached debugpy-1.8.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "Using cached ipython-8.30.0-py3-none-any.whl (820 kB)\n",
      "Using cached jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "Using cached jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Using cached matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Using cached pyzmq-26.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (860 kB)\n",
      "Using cached tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\n",
      "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Using cached platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
      "Using cached prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
      "Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
      "Using cached parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, six, pyzmq, pygments, psutil, prompt_toolkit, platformdirs, pexpect, parso, packaging, nest-asyncio, executing, decorator, debugpy, asttokens, stack_data, python-dateutil, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
      "Successfully installed asttokens-3.0.0 comm-0.2.2 debugpy-1.8.9 decorator-5.1.1 executing-2.1.0 ipykernel-6.29.5 ipython-8.30.0 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 packaging-24.2 parso-0.8.4 pexpect-4.9.0 platformdirs-4.3.6 prompt_toolkit-3.0.48 psutil-6.1.0 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.18.0 python-dateutil-2.9.0.post0 pyzmq-26.2.0 six-1.16.0 stack_data-0.6.3 tornado-6.4.2 traitlets-5.14.3 wcwidth-0.2.13\n",
      "Installed kernelspec chatbot in /home/vboxuser/.local/share/jupyter/kernels/chatbot\n",
      "Virtual Environment Created!\n"
     ]
    }
   ],
   "source": [
    "!python3 -m venv chatbot\n",
    "!chatbot/bin/pip install ipykernel\n",
    "!chatbot/bin/python -m ipykernel install --user --name=chatbot --display-name \"Python (chatbot)\"\n",
    "!source chatbot/bin/activate\n",
    "print('Virtual Environment Created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D5rh05lASA-"
   },
   "source": [
    "### 1.1 Requirements\n",
    "\n",
    "**Requirements**\n",
    "- Python >= 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YEy02uLUASA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Switch Kernel**\n",
    "\n",
    "Switch the Kernel in the Jupyter Notebook by\n",
    "- Go to the Menu Bar\n",
    "- Select Kernel\n",
    "- Select Change kernel\n",
    "- From the list of available kernels select `Python (chatbot)`\n",
    "- If `Python (chatbot)` is not available, refresh the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSmEdMfJASA_"
   },
   "source": [
    "### 1.2 Importing Required Libraries\n",
    "\n",
    "**Purpose**: Set up the environment, configure logging, suppress warnings, and import dependencies\n",
    "\n",
    "**Input**: None\n",
    "\n",
    "**Output**: Prints \"Dependencies imported successfully.\"\n",
    "\n",
    "**Processing**:\n",
    "- Configures logging with INFO level and standard format.\n",
    "- Disables Hugging Face tokenizers parallelism warnings.\n",
    "- Installs required dependencies silently using pip.\n",
    "- Suppresses non-critical warnings for clean output.\n",
    "- Imports essential libraries for vector store operations and document processing.\n",
    "- Confirms successful imports with a message.\n",
    "\n",
    "**Libraries**:\n",
    "\n",
    "- `os`: For interacting with the operating system and managing environment variables.\n",
    "\n",
    "- `warnings`: For suppressing warning messages during runtime.\n",
    "\n",
    "- `pymilvus.connections` and `pymilvus.utility`: For connecting to and managing the Milvus vector database.\n",
    "\n",
    "- `create_stuff_documents_chain` from `langchain.chains.combine_documents`: For combining documents into a chain.\n",
    "\n",
    "- `Document` from `langchain.schema`: For representing and managing document data structures.\n",
    "\n",
    "- `ChatPromptTemplate` from `langchain_core.prompts`: For creating prompts to structure queries for the language model.\n",
    "\n",
    "- `langchain-groq` from `langchain_groq.chat_models`: For integrating the GROQ AI language model.\n",
    "\n",
    "- `Milvus` from `langchain_milvus`: For using the Milvus vector database with LangChain.\n",
    "\n",
    "- `RecursiveUrlLoader` from `langchain_community.document_loaders`: For fetching and loading documents recursively from a URL.\n",
    "\n",
    "- `BeautifulSoup` from `bs4`: For parsing HTML and extracting relevant content.\n",
    "\n",
    "- `RecursiveCharacterTextSplitter` from `langchain_text_splitters`: For splitting large text data into smaller chunks for easier processing.\n",
    "\n",
    "- `create_retrieval_chain` from `langchain.chains`: For building chains to retrieve and combine relevant documents.\n",
    "\n",
    "- `HuggingFaceEmbeddings` from `langchain_huggingface`: For generating embeddings using Hugging Face models to encode text into vectors.\n",
    "\n",
    "- `HTTPStatusError` from `httpx`: For handling HTTP status-related exceptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LuMMd9ROASA_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Suppress Hugging Face tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Installing dependencies if not already installed, suppressing \"Requirement already satisfied\" warnings\n",
    "!chatbot/bin/pip install -q httpx pymilvus python-dotenv --root-user-action=ignore\n",
    "!chatbot/bin/pip install -q langchain langchain-core langchain-milvus langchain-groq langchain-community beautifulsoup4 langchain-text-splitters langchain-huggingface sentence-transformers --root-user-action=ignore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "from dotenv import set_key, load_dotenv\n",
    "\n",
    "print(\"Dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuOtuMhtASA_"
   },
   "source": [
    "### 1.3 Set Up Environment Variables\n",
    "\n",
    "**Purpose**:\n",
    "\n",
    "To define and load the environment variables required for the RAG (Retrieval-Augmented Generation) operation, including settings for the corpus source, API key, vector store database, and embedding model.\n",
    "\n",
    "**Input**:\n",
    "Prompted for GROQ API that can be found [here](https://console.groq.com/keys)\n",
    "\n",
    "1. The environment variable `GROQ_API_KEY` being set in the system/environment.\n",
    "\n",
    "**Output**:\n",
    "1. Four variables are defined and available for subsequent code execution:\n",
    "\n",
    "- `CORPUS_SOURCE`: URL of the corpus to be processed [default](https://www.csusb.edu/its).\n",
    "- `GROQ_API_KEY`: API key for authenticating with the MistralAI service, fetched from system environment variables.\n",
    "- `MILVUS_URI`: File path for the Milvus Lite vector database (default: `'milvus/jupyter_milvus_vector.db'`).\n",
    "- `MODEL_NAME`: Name of the embedding model for vectorizing documents (default: `'sentence-transformers/all-MiniLM-L12-v2'`).\n",
    "2. Prints \"ENV variables defined.\" to indicate successful setup.\n",
    "\n",
    "**Processing**:\n",
    "1. **Set the Corpus Source**:\n",
    "\n",
    "- Assigns the URL (`https://www.csusb.edu/its`) to the variable `CORPUS_SOURCE`.\n",
    "- This specifies the website from which documents will be loaded for processing.\n",
    "\n",
    "2. **Fetch GROQ API Key**:\n",
    "\n",
    "- Retrieves the API key for GROQ AI using `os.environ.get(\"GROQ_API_KEY\")`.\n",
    "- This ensures secure access by fetching sensitive credentials from the system's environment variables.\n",
    "3. **Set the Milvus Database Path**:\n",
    "\n",
    "- Defines the file path (`milvus/jupyter_milvus_vector.db`) for the Milvus Lite vector store via `MILVUS_URI`.\n",
    "- This will be used to store and retrieve vector embeddings.\n",
    "4. **Set the Embedding Model Name**:\n",
    "\n",
    "- Specifies the name of the Hugging Face embedding model (`sentence-transformers/all-MiniLM-L12-v2`) in `MODEL_NAME`.\n",
    "- This model is used for transforming text into vector embeddings for document processing.\n",
    "5. **Print Status**:\n",
    "\n",
    "- Prints a confirmation message (`\"ENV variables defined.\"`) to confirm successful setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GaImJ3wgASBA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your GROQ API key: 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\n",
      "No API key entered. Operation canceled.\n",
      "ENV variables defined.\n"
     ]
    }
   ],
   "source": [
    "env_file = \".env\"\n",
    "\n",
    "# Load existing environment variables from .env if it exists\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "api_key = input(\"Please enter your GROQ API key: \").strip()\n",
    "\n",
    "# Save the API key to the .env file\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    set_key(env_file, \"GROQ_API_KEY\", api_key)\n",
    "else:\n",
    "    print(\"No API key entered. Operation canceled.\")\n",
    "    \n",
    "CORPUS_SOURCE = 'https://www.csusb.edu/its'\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "MILVUS_URI = \"milvus/jupyter_milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "print(\"ENV variables defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JrV052RASBA"
   },
   "source": [
    "## 2.Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrV5R7TrASBA"
   },
   "source": [
    "### 2.1 Vector Store and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gr_4BPVASBA"
   },
   "source": [
    "#### 2.1.1 Function to Check Vector Store (Milvus database)\n",
    "\n",
    "**Purpose**:\n",
    "To check if the Milvus vector store already exists at the specified URI.\n",
    "\n",
    "**Input**: Path to the Milvus database `uri` (str).\n",
    "\n",
    "**Output**:\n",
    "Returns a boolean (True if the vector store exists, False otherwise).\n",
    "\n",
    "**Processing**:\n",
    "- Creates the `/milvus` directory if it doesn’t exist.\n",
    "- Connects to the Milvus database at the specified uri.\n",
    "- Checks if the collection IT_support exists in the Milvus database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aKw6ws8VASBA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `vector_store_check` defined.\n"
     ]
    }
   ],
   "source": [
    "def vector_store_check(uri):\n",
    "    \"\"\"\n",
    "    Returns response on whether the vector storage exists\n",
    "\n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "\n",
    "    # Return True if exists, False otherwise\n",
    "    return utility.has_collection(\"IT_support\")\n",
    "\n",
    "print(\"Function `vector_store_check` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqyTX7WsASBA"
   },
   "source": [
    "#### 2.1.2 Function to fetch the embedding model\n",
    "\n",
    "**Purpose**:\n",
    "To load and initialize the embedding model for vectorizing documents.\n",
    "\n",
    "**Input**:\n",
    "None.\n",
    "\n",
    "**Output**: Returns the embedding function loaded from the Hugging Face model specified in `MODEL_NAME`.\n",
    "\n",
    "**Processing**:\n",
    "- Loads the embedding model using `HuggingFaceEmbeddings`.\n",
    "- Returns the initialized embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EC9u5DryASBA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `get_embedding_function` defined.\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "    return embedding_function\n",
    "\n",
    "print(\"Function `get_embedding_function` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl5Jkd_CASBA"
   },
   "source": [
    "### 2.2. Document Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMnXlFYZASBA"
   },
   "source": [
    "#### 2.2.1 Function to Clean Text\n",
    "**Purpose**:\n",
    "To clean a given text by removing extra whitespace and blank lines.\n",
    "\n",
    "**Input**: The input text to be cleaned `text` (str).\n",
    "\n",
    "**Output**:\n",
    "Returns the cleaned text with unnecessary whitespace and blank lines removed.\n",
    "\n",
    "**Processing**:\n",
    "- Splits the text into lines and trims leading/trailing spaces from each line.\n",
    "- Removes empty lines from the text.\n",
    "- Joins the cleaned lines into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3C0PpETOASBB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `clean_text` defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Further clean the text by removing extra whitespace and new lines.\"\"\"\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    cleaned_lines = [line for line in lines if line]\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "print(\"Function `clean_text` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4OhiuMxASBB"
   },
   "source": [
    "#### 2.2.2 Function to Clean and Extract Text from HTML Content\n",
    "**Purpose**:\n",
    "To extract and clean the main content from an HTML document.\n",
    "\n",
    "**Input**: The HTML content to be cleaned `html_content` (str).\n",
    "\n",
    "**Output**:\n",
    "Returns the cleaned plain text content extracted from the HTML.\n",
    "\n",
    "**Processing**:\n",
    "\n",
    "- Parses the HTML using `BeautifulSoup`.\n",
    "- Removes unnecessary elements like `<script>, <style>, <header>, <footer>, and <nav>`.\n",
    "- Extracts text from the `<main>` tag if it exists, or the entire document otherwise.\n",
    "- Cleans the extracted text using the clean_text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mRqWraDmASBB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `clean_text_from_html` defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_text_from_html(html_content):\n",
    "    \"\"\"Clean HTML content to extract main text.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove unnecessary elements\n",
    "    for script_or_style in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    main_content = soup.find('main')\n",
    "    if main_content:\n",
    "        content = main_content.get_text(separator='\\n')\n",
    "    else:\n",
    "        content = soup.get_text(separator='\\n')\n",
    "\n",
    "    return clean_text(content)\n",
    "\n",
    "print(\"Function `clean_text_from_html` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki9hACtjASBB"
   },
   "source": [
    "#### 2.2.3 Function for loading documents from the web\n",
    "\n",
    "**Purpose**:\n",
    "To recursively load and clean documents from a web source specified in `CORPUS_SOURCE`.\n",
    "\n",
    "**Input**:\n",
    "None.\n",
    "\n",
    "**Output**:\n",
    "Returns a list of cleaned documents as `Document` objects.\n",
    "\n",
    "**Processing**:\n",
    "\n",
    "- Uses RecursiveUrlLoader to load all pages from the base URL (`CORPUS_SOURCE`).\n",
    "- Iterates through the loaded documents:\n",
    "- Cleans the text using `clean_text_from_html`.\n",
    "- Creates `Document` objects with the cleaned text and metadata.\n",
    "- Returns the list of cleaned `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d0YJWcSvASBB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `load_documents_from_web` defined.\n"
     ]
    }
   ],
   "source": [
    "def load_documents_from_web():\n",
    "    \"\"\"\n",
    "    Load the documents from the web and store the page contents\n",
    "\n",
    "    Returns:\n",
    "        list: The documents loaded from the web\n",
    "    \"\"\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=CORPUS_SOURCE,\n",
    "        prevent_outside=True,\n",
    "        base_url=CORPUS_SOURCE\n",
    "        )\n",
    "    raw_documents = loader.load()\n",
    "\n",
    "    # Ensure documents are cleaned\n",
    "    cleaned_documents = []\n",
    "    for doc in raw_documents:\n",
    "        cleaned_text = clean_text_from_html(doc.page_content)\n",
    "        cleaned_documents.append(Document(page_content=cleaned_text, metadata=doc.metadata))\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "print(\"Function `load_documents_from_web` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNyhOMEFASBB"
   },
   "source": [
    "### 2.3 Milvus Vector Store Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qy_IO5IASBB"
   },
   "source": [
    "#### 2.3.1 Function to load existing vector store (Milvus database)\n",
    "\n",
    "**Purpose**: To connect to an existing Milvus vector store.  \n",
    "\n",
    "**Input**: Path to the Milvus database (`uri`).  \n",
    "\n",
    "**Output**: Returns the loaded vector store as a Milvus object.  \n",
    "\n",
    "**Processing**:\n",
    "- Connects to the specified Milvus database.\n",
    "- Initializes a Milvus object with the collection IT_support and the embedding function from `get_embedding_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4B_P_f4JASBB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `load_existing_db` defined.\n"
     ]
    }
   ],
   "source": [
    "def load_existing_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Load an existing vector store\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"IT_support\",\n",
    "        embedding_function=get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "\n",
    "    print(\"Vector store loaded\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `load_existing_db` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4dnlGSfASBB"
   },
   "source": [
    "#### 2.3.2 Function to split documents\n",
    "\n",
    "**Purpose**: To split large documents into smaller chunks for better processing and context preservation.  \n",
    "\n",
    "**Input**: List of documents to be split.  \n",
    "\n",
    "**Output**: Returns a list of document chunks.  \n",
    "\n",
    "**Processing**:\n",
    "\n",
    "- Creates a `RecursiveCharacterTextSplitter` with a chunk size of 1000 characters and an overlap of 300 characters.\n",
    "- Splits each document into smaller chunks using the text splitter.\n",
    "- Returns the list of document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wpHoV-ZeASBC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `split_documents` defined.\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split the documents into chunks\n",
    "\n",
    "    Args:\n",
    "        documents (list): The documents to split\n",
    "\n",
    "    Returns:\n",
    "        list: list of chunks of documents\n",
    "    \"\"\"\n",
    "    # Create a text splitter to split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(\"Documents successfully split\")\n",
    "    return docs\n",
    "\n",
    "print(\"Function `split_documents` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2X_1PkVASBC"
   },
   "source": [
    "#### 2.3.3 Function to Create New Vector Store (Milvus database)\n",
    "\n",
    "**Purpose**:\n",
    "To create a new vector store in Milvus using the given documents and embedding function.\n",
    "\n",
    "**Input(s)**:\n",
    "\n",
    "- `docs` (list): Documents to store in the vector database.\n",
    "- `embeddings`: The embedding function for vectorizing documents.\n",
    "- `uri` (str): Path to the Milvus database.  \n",
    "\n",
    "**Output**: Returns the newly created vector store.  \n",
    "\n",
    "**Processing**:\n",
    "\n",
    "- Connects to Milvus at the specified uri.\n",
    "- Creates a new collection named IT_support, dropping any existing one.\n",
    "- Embeds the documents and stores them in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZFgsj3gjASBC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `create_vector_store` defined.\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create a new vector store and drop any existing one\n",
    "    vector_store = Milvus.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"IT_support\",\n",
    "        connection_args={\"uri\": uri},\n",
    "        drop_old=True,\n",
    "    )\n",
    "\n",
    "    print(\"Vector store created\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `create_vector_store` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y6TNg2eASBC"
   },
   "source": [
    "#### 2.3.4 Core function for initializing Milvus\n",
    "\n",
    "**Purpose**: To initialize the Milvus vector store by either loading an existing one or creating a new one.\n",
    "\n",
    "**Input**: Path to the Milvus database `uri`.  \n",
    "\n",
    "**Output**: Returns the initialized vector store.  \n",
    "\n",
    "**Processing**:\n",
    "- Checks if the vector store already exists using vector_store_check.\n",
    "- If it exists:  \n",
    "    - loads it using `load_existing_db`.  \n",
    "- Otherwise:\n",
    "    - Loads documents from the web using `load_documents_from_web`.\n",
    "    - Splits the documents into chunks using `split_documents`.\n",
    "    - Creates a new vector store using `create_vector_store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oMZJ-2slASBC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `initialize_milvus` defined.\n"
     ]
    }
   ],
   "source": [
    "def initialize_milvus(uri: str=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Initialize the vector store for the RAG model\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local vector storage. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    if vector_store_check(uri):\n",
    "        vector_store = load_existing_db(uri)\n",
    "        print(\"Embeddings loaded from existing storage\")\n",
    "    else:\n",
    "        embeddings = get_embedding_function()\n",
    "        print(\"Embeddings Loaded\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(\"Documents Loaded\")\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        docs = split_documents(documents=documents)\n",
    "        print(\"Documents Splitting completed\")\n",
    "\n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "    print(\"Milvus successfully initialized\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `initialize_milvus` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKaV-fRsASBC"
   },
   "source": [
    "#### 2.3.5 Initializing vector store (Milvus database)\n",
    "**Purpose**: To initialize the Milvus database system for vector storage and retrieval.  \n",
    "\n",
    "**Input**: None.  \n",
    "\n",
    "**Output**: None.  \n",
    "\n",
    "**Processing**: Calls the `initialize_milvus` function to set up the Milvus vector database system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "io9WEIxvASBC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Milvus initialization.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/chatbot/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:52\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Downloads/chatbot/lib/python3.12/site-packages/sentence_transformers/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n",
      "File \u001b[0;32m~/Downloads/chatbot/lib/python3.12/site-packages/sentence_transformers/backend.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[1;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/chatbot/lib/python3.12/site-packages/sentence_transformers/util.py:19\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download, snapshot_download\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Milvus initialization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43minitialize_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36minitialize_milvus\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings loaded from existing storage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings Loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     documents \u001b[38;5;241m=\u001b[39m load_documents_from_web()\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mget_embedding_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding_function\u001b[39m():\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Returns embedding function for the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        embedding function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     embedding_function \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding_function\n",
      "File \u001b[0;32m~/Downloads/chatbot/lib/python3.12/site-packages/langchain_huggingface/embeddings/huggingface.py:54\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[1;32m     61\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "print(\"Starting Milvus initialization.\")\n",
    "initialize_milvus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuHcXZCdASBC"
   },
   "source": [
    "## 3. Testing the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DAXDSsWASBD"
   },
   "source": [
    "### 3.1 Function to create RAG prompt\n",
    "\n",
    "**Purpose**:\n",
    "To create a prompt template for the RAG model with predefined system instructions\n",
    "\n",
    "**Input**: None\n",
    "\n",
    "**Output**: Returns a `ChatPromptTemplate` object\n",
    "\n",
    "**Processing**:\n",
    "- Defines a template with system instructions for generating accurate and context-based responses\n",
    "- Initializes a `ChatPromptTemplate` using the system template and a human prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "levo2gTHASBD"
   },
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Create a prompt template for the RAG model\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The prompt template for the RAG model\n",
    "    \"\"\"\n",
    "    # Define the prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\\\n",
    "    You are an AI assistant that provides answers strictly based on the provided context. Adhere to these guidelines:\n",
    "     - Only answer questions based on the content within the <context> tags.\n",
    "     - If the <context> does not contain information related to the question, respond only with: \"I don't have enough information to answer this question.\"\n",
    "     - For unclear questions or questions that lack specific context, request clarification from the user.\n",
    "     - Provide specific, concise ansewrs. Where relevant information includes statistics or numbers, include them in the response.\n",
    "     - Avoid adding any information, assumption, or external knowledge. Answer accurately within the scope of the given context and do not guess.\n",
    "     - If information is missing, respond only with: \"I don't have enough information to answer this question.\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", PROMPT_TEMPLATE),\n",
    "        (\"human\", \"<question>{input}</question>\\n\\n<context>{context}</context>\"),\n",
    "    ])\n",
    "\n",
    "    print(\"Prompt template defined\")\n",
    "    return prompt\n",
    "\n",
    "print(\"Function `create_prompt` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkLn8RvRASBD"
   },
   "source": [
    "### 3.2 Function to query RAG model\n",
    "\n",
    "**Purpose**: To query the RAG model for a response to a user’s question.\n",
    "\n",
    "**Input**: User’s question query.\n",
    "\n",
    "**Output**: Returns the generated response from the RAG model, including source references.\n",
    "\n",
    "**Processing**:\n",
    "\n",
    "- Initializes the MistralAI model.\n",
    "- Loads the prompt template using `create_prompt`.\n",
    "- Loads the vector store and creates a retriever to fetch relevant documents.\n",
    "- Creates a document chain and a retrieval chain for generating responses.\n",
    "- Executes the query and generates a response using the retrieval chain.\n",
    "- Extracts and appends source metadata to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnLO-7RPASBD"
   },
   "outputs": [],
   "source": [
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store,\n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatGroq(model='llama-3.1-70b-versatile', temperature = 0)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\n",
    "    print(\"Model Loaded\")\n",
    "\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)\n",
    "    retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.7, \"k\":5})\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)\n",
    "        print(\"Document Chain Created\")\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        print(\"Retrieval Chain Created\")\n",
    "\n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain.invoke({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:\n",
    "        print(f\"HTTPStatusError: {e}\")\n",
    "        if e.response.status_code == 429:\n",
    "            error_message = \"I am currently experiencing high traffic. Please try again later.\"\n",
    "            print(error_message)\n",
    "            return error_message, []\n",
    "        error_message = \"I am unable to answer this question at the moment. Please try again later.\"\n",
    "        print(error_message)\n",
    "        return error_message, []\n",
    "\n",
    "    # logic to add sources to the response\n",
    "    max_relevant_sources = 4 # number of sources at most to be added to the response\n",
    "    all_sources = \"\"\n",
    "    sources = []\n",
    "    count = 1\n",
    "    for i in range(max_relevant_sources):\n",
    "        try:\n",
    "            source = response[\"context\"][i].metadata[\"source\"]\n",
    "            # check if the source is already added to the list\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "                all_sources += f\"[Source {count}]({source}), \"\n",
    "                count += 1\n",
    "        except IndexError: # if there are no more sources to add\n",
    "            break\n",
    "    all_sources = all_sources[:-2] # remove the last comma and space\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Response Generated:\\n\")\n",
    "\n",
    "    return response[\"answer\"]\n",
    "\n",
    "print(\"Function `query_rag` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S06p6wUnASBD"
   },
   "source": [
    "### 3.3 Get response from RAG\n",
    "\n",
    "**Purpose**: Taking input query from user, executing it and displaying the fetched output.  \n",
    "\n",
    "**Input**: User's question.\n",
    "\n",
    "**Output**: Response generated from the `query_rag` function.\n",
    "\n",
    "**Processing**:\n",
    "- User input: Prompts the user to input a query.\n",
    "- Query execution: Passes the user-provided query to the `query_rag` function.\n",
    "- Response display: Prints the response generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq4aNLCGASBD"
   },
   "outputs": [],
   "source": [
    "response = query_rag(input(\"Enter your query: \"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8XhfVgoASBD"
   },
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww_Yv6MoASBD"
   },
   "source": [
    " **Recap**:\n",
    "\n",
    "- Developed an **ITS Support chatbot** using the **RAG (Retrieval-Augmented Generation)** system, which retrieves relevant documents and generates responses based on the context provided.\n",
    "- Integrated **widgets** in **Jupyter Notebook** for interactive user input and response display, enabling real-time query processing.\n",
    "- Configured the chatbot with **natural language processing models** like **GROQ AI** and **vector-based document retrieval** using **Milvus**, ensuring accurate and context-aware responses.\n",
    "\n",
    "**Next Steps**:\n",
    "\n",
    "- Expand the chatbot's knowledge base to accommodate a broader range of questions or integrate multiple datasets from diverse sources, enhancing the system’s robustness and versatility.\n",
    "- Improve the chatbot’s performance and extend its capabilities by incorporating more advanced features, such as deeper integrations with external knowledge sources and enhanced response-generation techniques.\n",
    "\n",
    "**Resources**:\n",
    "\n",
    "- \"The **ITS Support Chatbot** project was developed by **CSUSB Fall 2024 CSE6550 Team 1**  to provide automated assistance for IT-related queries and technical support.\"\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1)\n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1/wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
