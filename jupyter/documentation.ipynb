{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITS Support Chatbot\n",
    "\n",
    "This chatbot is an educational tool that's built to answer questions related to the CSUSB's [Information Technology Services](https://www.csusb.edu/its). The chatbot was built by team 1 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
    "\n",
    "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the ITS website as the primary data source.\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1) \n",
    "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1/wiki)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#1-Setup)  \n",
    "    - 1.1. [Import Required Libraries](#1.1-Importing-Required-Libraries)  \n",
    "    - 1.2. [Set Up Environment Variables](#1.2-Set-Up-Environment-Variables)  \n",
    "2. [Building the Chatbot](#2.-Building-the-Chatbot)  \n",
    "    - 2.1 [Vector Store and Embeddings](#2.1-Vector-Store-and-Embeddings)\n",
    "        - 2.1.1. [Create Vector Store](#2.1.1-Function-to-fetch-the-embedding-model)  \n",
    "        - 2.1.2. [Fetch Embedding Model](#2.1.2-Function-to-fetch-the-embedding-model)  \n",
    "    - 2.2. [Document Handling](#2.-Document-Handling)  \n",
    "        - 2.2.1. [Text Cleaning](#2.1-Function-to-Clean-Text)  \n",
    "        - 2.2.2. [Clean HTML Content](#2.2-Function-to-Clean-and-Extract-Text-from-HTML-Content)  \n",
    "        - 2.2.3. [Load Documents from the Web](#2.3-Function-for-loading-documents-from-the-web) \n",
    "    - 2.3 [Milvus Vector Store Management](#2.3.-Milvus-Vector-Store-Management) \n",
    "        - 2.3.1. [Load Existing Vector Store](#2.3.1-Function-to-load-existing-vector-store-(Milvus-database))\n",
    "        - 2.3.2. [Split Documents into Chunks](#2.3.2-Function-to-split-documents)   \n",
    "        - 2.3.3. [Create New Vector Store](#2.3.3-Function-to-Create-New-Vector-Store-(Milvus-database))  \n",
    "        - 2.3.4. [Initialize Milvus](#2.3.4-Core-function-for-initializing-Milvus) \n",
    "        - 2.3.5. [Initializing Vector Store](#2.3.5-Initializing-Vector-Store)\n",
    "3. [Testing the Chatbot](#3.-Testing-the-Chatbot)  \n",
    "    - 3.1. [Create RAG Prompt](#3.1-Function-to-create-RAG-prompt)  \n",
    "    - 3.2. [Query RAG](#3.2-Function-to-query-RAG-model)  \n",
    "    - 3.3. [Retrieve RAG Response](#3.3-Get-response-from-RAG)  \n",
    "4. [Contributors](#4-Contributors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Required Libraries\n",
    "\n",
    "Importing core libraries: dotenv for environment management, requests and httpx for HTTP requests, pymilvus for vector storage and langchain.Extensions of langchain include core, mistralai, milvus, community, text-splitters and huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level to INFO\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress Hugging Face tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Installing dependencies if not already installed, suppressing \"Requirement already satisfied\" warnings\n",
    "!pip install -q httpx pymilvus --root-user-action=ignore\n",
    "!pip install -q langchain langchain-core langchain-mistralai langchain-cohere langchain-milvus langchain-community beautifulsoup4 langchain-text-splitters langchain-huggingface --root-user-action=ignore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "print(\"Dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set Up Environment Variables\n",
    "\n",
    "\n",
    "Load the necessary environment variables for RAG operation. `CORPUS_SOURCE` can be modified to load a different corpus, `MISTRAL_API_KEY` stores the MistralAI API key, `MILVUS_URI` provides the path for the milvus lite database file, and `MODEL_NAME` specifies the embedding model for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_SOURCE = 'https://www.csusb.edu/its'\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "MILVUS_URI = \"milvus/jupyter_milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "print(\"ENV variables defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Building the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vector Store and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Function to Check Vector Store (Milvus database)\n",
    "\n",
    "Purpose: \n",
    "To check if the Milvus vector store already exists at the specified URI.\n",
    "\n",
    "Input: Path to the Milvus database `uri` (str).\n",
    "\n",
    "Output: \n",
    "Returns a boolean (True if the vector store exists, False otherwise).\n",
    "\n",
    "Processing:\n",
    "- Creates the /milvus directory if it doesnâ€™t exist.\n",
    "- Connects to the Milvus database at the specified uri.\n",
    "- Checks if the collection IT_support exists in the Milvus database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_store_check(uri):\n",
    "    \"\"\"\n",
    "    Returns response on whether the vector storage exists\n",
    "\n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    \n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "\n",
    "    # Return True if exists, False otherwise\n",
    "    return utility.has_collection(\"IT_support\")\n",
    "\n",
    "print(\"Function `vector_store_check` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Function to fetch the embedding model\n",
    "\n",
    "Purpose:\n",
    "To load and initialize the embedding model for vectorizing documents.\n",
    "\n",
    "Input:\n",
    "None.\n",
    "\n",
    "Output: Returns the embedding function loaded from the Hugging Face model specified in `MODEL_NAME`.\n",
    "\n",
    "Processing:\n",
    "- Loads the embedding model using `HuggingFaceEmbeddings`.\n",
    "- Returns the initialized embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    \n",
    "    return embedding_function\n",
    "\n",
    "print(\"Function `get_embedding_function` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Document Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Function to Clean Text\n",
    "Purpose:\n",
    "To clean a given text by removing extra whitespace and blank lines.\n",
    "\n",
    "Input:\n",
    "- `text` (str): The input text to be cleaned.\n",
    "\n",
    "Output:\n",
    "Returns the cleaned text with unnecessary whitespace and blank lines removed.\n",
    "\n",
    "Processing:\n",
    "- Splits the text into lines and trims leading/trailing spaces from each line.\n",
    "- Removes empty lines from the text.\n",
    "- Joins the cleaned lines into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Further clean the text by removing extra whitespace and new lines.\"\"\"\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    cleaned_lines = [line for line in lines if line]\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "print(\"Function `clean_text` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Function to Clean and Extract Text from HTML Content\n",
    "Purpose:\n",
    "To extract and clean the main content from an HTML document.\n",
    "\n",
    "Input:\n",
    "- `html_content` (str): The HTML content to be cleaned.\n",
    "\n",
    "Output:\n",
    "Returns the cleaned plain text content extracted from the HTML.\n",
    "\n",
    "Processing:\n",
    "\n",
    "- Parses the HTML using `BeautifulSoup`.\n",
    "- Removes unnecessary elements like `<script>, <style>, <header>, <footer>, and <nav>`.\n",
    "- Extracts text from the `<main>` tag if it exists, or the entire document otherwise.\n",
    "- Cleans the extracted text using the clean_text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_from_html(html_content):\n",
    "    \"\"\"Clean HTML content to extract main text.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove unnecessary elements\n",
    "    for script_or_style in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    main_content = soup.find('main')\n",
    "    if main_content:\n",
    "        content = main_content.get_text(separator='\\n')\n",
    "    else:\n",
    "        content = soup.get_text(separator='\\n')\n",
    "\n",
    "    return clean_text(content)\n",
    "\n",
    "print(\"Function `clean_text_from_html` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Function for loading documents from the web\n",
    "\n",
    "Purpose:\n",
    "To recursively load and clean documents from a web source specified in `CORPUS_SOURCE`.\n",
    "\n",
    "Input:\n",
    "None.\n",
    "\n",
    "Output:\n",
    "Returns a list of cleaned documents as `Document` objects.\n",
    "\n",
    "Processing:\n",
    "\n",
    "- Uses RecursiveUrlLoader to load all pages from the base URL (`CORPUS_SOURCE`).\n",
    "- Iterates through the loaded documents:\n",
    "- Cleans the text using `clean_text_from_html`.\n",
    "- Creates `Document` objects with the cleaned text and metadata.\n",
    "- Returns the list of cleaned `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_web():\n",
    "    \"\"\"\n",
    "    Load the documents from the web and store the page contents\n",
    "\n",
    "    Returns:\n",
    "        list: The documents loaded from the web\n",
    "    \"\"\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=CORPUS_SOURCE,\n",
    "        prevent_outside=True,\n",
    "        base_url=CORPUS_SOURCE\n",
    "        )\n",
    "    raw_documents = loader.load()\n",
    "    \n",
    "    # Ensure documents are cleaned\n",
    "    cleaned_documents = []\n",
    "    for doc in raw_documents:\n",
    "        cleaned_text = clean_text_from_html(doc.page_content)\n",
    "        cleaned_documents.append(Document(page_content=cleaned_text, metadata=doc.metadata))\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "print(\"Function `load_documents_from_web` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Milvus Vector Store Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Function to load existing vector store (Milvus database)\n",
    "\n",
    "Accepts the path to the database and embedding function to establish a connection with the database, returning the connected vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Load an existing vector store\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"IT_support\",\n",
    "        embedding_function=get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Vector store loaded\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `load_existing_db` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Function to split documents\n",
    "\n",
    "Takes the documents loaded from `load_documents_from_web` and splits them into chunks of 1000 characters. Overlaps 300 characters to preserve context. Returns the documents split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split the documents into chunks\n",
    "\n",
    "    Args:\n",
    "        documents (list): The documents to split\n",
    "\n",
    "    Returns:\n",
    "        list: list of chunks of documents\n",
    "    \"\"\"\n",
    "    # Create a text splitter to split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    logger.info(\"Documents successfully split\")\n",
    "    return docs\n",
    "\n",
    "print(\"Function `split_documents` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Function to Create New Vector Store (Milvus database)\n",
    "\n",
    "Uses the documents retrieved from `load_documents_from_web`, the embedding function from `get_embedding_function`, and the database path to establish the vector store. It returns the vector store once it has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create a new vector store and drop any existing one\n",
    "    vector_store = Milvus.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"IT_support\",\n",
    "        connection_args={\"uri\": uri},\n",
    "        drop_old=True,\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Vector store created\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `create_vector_store` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Core function for initializing Milvus\n",
    "\n",
    "This is the primary function for initializing Milvus, utilizing the previously defined functions to fully set up the vector store. Executing `initialize_milvus` will invoke all the necessary functions required for creating the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_milvus(uri: str=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Initialize the vector store for the RAG model\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local vector storage. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    if vector_store_check(uri):\n",
    "        vector_store = load_existing_db(uri)\n",
    "        logger.info(\"Embeddings loaded from existing storage\")\n",
    "    else:\n",
    "        embeddings = get_embedding_function()\n",
    "        logger.info(\"Embeddings Loaded\")\n",
    "        documents = load_documents_from_web()\n",
    "        logger.info(\"Documents Loaded\")\n",
    "    \n",
    "        # Split the documents into chunks\n",
    "        docs = split_documents(documents=documents)\n",
    "        logger.info(\"Documents Splitting completed\")\n",
    "    \n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "    logger.info(\"Milvus successfully initialized\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `initialize_milvus` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Initializing vector store (Milvus database)\n",
    "\n",
    "This process may take a considerable amount of time due to the embedding function. Please be patient while it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Milvus initialization.\")\n",
    "initialize_milvus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Function to create RAG prompt\n",
    "\n",
    "Purpose:\n",
    "To create a prompt template for the RAG model with predefined system instructions.\n",
    "\n",
    "Input:\n",
    "None.\n",
    "\n",
    "Output:\n",
    "Returns a `ChatPromptTemplate` object.\n",
    "\n",
    "Processing:\n",
    "- Defines a template with system instructions for generating accurate and context-based responses.\n",
    "- Initializes a `ChatPromptTemplate` using the system template and a human prompt structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Create a prompt template for the RAG model\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The prompt template for the RAG model\n",
    "    \"\"\"\n",
    "    # Define the prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\\\n",
    "    You are an AI assistant that provides answers strictly based on the provided context. Adhere to these guidelines:\n",
    "     - Only answer questions based on the content within the <context> tags.\n",
    "     - If the <context> does not contain information related to the question, respond only with: \"I don't have enough information to answer this question.\"\n",
    "     - For unclear questions or questions that lack specific context, request clarification from the user.\n",
    "     - Provide specific, concise ansewrs. Where relevant information includes statistics or numbers, include them in the response.\n",
    "     - Avoid adding any information, assumption, or external knowledge. Answer accurately within the scope of the given context and do not guess.\n",
    "     - If information is missing, respond only with: \"I don't have enough information to answer this question.\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", PROMPT_TEMPLATE),\n",
    "        (\"human\", \"<question>{input}</question>\\n\\n<context>{context}</context>\"),\n",
    "    ])\n",
    "\n",
    "    logger.info(\"Prompt template defined\")\n",
    "    return prompt\n",
    "\n",
    "print(\"Function `create_prompt` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Function to query RAG model\n",
    "\n",
    "Loads the MistralAI model, the prompt template, and the vector store (Milvus database). Converts the vector store into a retriever that fetches documents containing relevant context. Constructs a document chain that includes all context-related documents. Creates a retrieval chain that utilizes the documents and retriever to gather context based on the user's question. Additionally, retrieves source metadata from the context documents that have been fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatMistralAI(model='open-mistral-7b')\n",
    "    logger.info(\"Model Loaded\")\n",
    "\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)\n",
    "    retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.7, \"k\":5})\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)\n",
    "        logger.info(\"Document Chain Created\")\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        logger.info(\"Retrieval Chain Created\")\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain.invoke({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:\n",
    "        logger.error(f\"HTTPStatusError: {e}\")\n",
    "        if e.response.status_code == 429:\n",
    "            error_message = \"I am currently experiencing high traffic. Please try again later.\"\n",
    "            logger.error(error_message)\n",
    "            return error_message, []\n",
    "        error_message = \"I am unable to answer this question at the moment. Please try again later.\"\n",
    "        logger.error(error_message)\n",
    "        return error_message, []\n",
    "    \n",
    "    # logic to add sources to the response\n",
    "    max_relevant_sources = 4 # number of sources at most to be added to the response\n",
    "    all_sources = \"\"\n",
    "    sources = []\n",
    "    count = 1\n",
    "    for i in range(max_relevant_sources):\n",
    "        try:\n",
    "            source = response[\"context\"][i].metadata[\"source\"]\n",
    "            # check if the source is already added to the list\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "                all_sources += f\"[Source {count}]({source}), \"\n",
    "                count += 1\n",
    "        except IndexError: # if there are no more sources to add\n",
    "            break\n",
    "    all_sources = all_sources[:-2] # remove the last comma and space\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Response Generated:\\n\")\n",
    "    \n",
    "    return response[\"answer\"]\n",
    "\n",
    "print(\"Function `query_rag` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get response from RAG\n",
    "\n",
    "Send a question to RAG, retrieve the response, and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_rag(input(\"Enter your query: \"))\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
