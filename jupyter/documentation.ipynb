{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pViAbjuYASA9"
      },
      "source": [
        "# IT Support Chatbot\n",
        "\n",
        "This chatbot is an educational tool that's built to answer questions related to the CSUSB's [Information Technology Services](https://www.csusb.edu/its). The chatbot was built by team 1 for [CSE 6550: Software Engineering Concepts](https://catalog.csusb.edu/coursesaz/cse/)\n",
        "\n",
        "In this notebook, we will demonstrate how the chatbot uses retrieval augemented generation (RAG) to answer questions using the ITS website as the primary data source.\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1)\n",
        "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1/wiki)\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup](#1-Setup)  \n",
        "    - 1.1. [Requirements and Environment](#1.1-Requirements-and-Environment)  \n",
        "    - 1.2. [Import Required Libraries](#1.2-Importing-Required-Libraries)  \n",
        "    - 1.3. [Set Up Environment Variables](#1.3-Set-Up-Environment-Variables)  \n",
        "2. [Building the Chatbot](#2.-Building-the-Chatbot)  \n",
        "    - 2.1 [Vector Store and Embeddings](#2.1-Vector-Store-and-Embeddings)\n",
        "        - 2.1.1. [Create Vector Store](#2.1.1-Function-to-fetch-the-embedding-model)  \n",
        "        - 2.1.2. [Fetch Embedding Model](#2.1.2-Function-to-fetch-the-embedding-model)  \n",
        "    - 2.2. [Document Handling](#2.-Document-Handling)  \n",
        "        - 2.2.1. [Text Cleaning](#2.1-Function-to-Clean-Text)  \n",
        "        - 2.2.2. [Clean HTML Content](#2.2-Function-to-Clean-and-Extract-Text-from-HTML-Content)  \n",
        "        - 2.2.3. [Load Documents from the Web](#2.3-Function-for-loading-documents-from-the-web)\n",
        "    - 2.3 [Milvus Vector Store Management](#2.3.-Milvus-Vector-Store-Management)\n",
        "        - 2.3.1. [Load Existing Vector Store](#2.3.1-Function-to-load-existing-vector-store-(Milvus-database))\n",
        "        - 2.3.2. [Split Documents into Chunks](#2.3.2-Function-to-split-documents)   \n",
        "        - 2.3.3. [Create New Vector Store](#2.3.3-Function-to-Create-New-Vector-Store-(Milvus-database))  \n",
        "        - 2.3.4. [Initialize Milvus](#2.3.4-Core-function-for-initializing-Milvus)\n",
        "        - 2.3.5. [Initializing Vector Store](#2.3.5-Initializing-Vector-Store)\n",
        "3. [Testing the Chatbot](#3.-Testing-the-Chatbot)  \n",
        "    - 3.1. [Create RAG Prompt](#3.1-Function-to-create-RAG-prompt)  \n",
        "    - 3.2. [Query RAG](#3.2-Function-to-query-RAG-model)  \n",
        "    - 3.3. [Retrieve RAG Response](#3.3-Get-response-from-RAG)  \n",
        "4. [Conclusion](#4-Conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpLCQScOASA-"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTiY7XcqASA_"
      },
      "source": [
        "Environment Installation:\n",
        "- Install ipykernel and virtualenv\n",
        "- Activate new virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwcflKxFASA_"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install -y python3.10-venv\n",
        "!python3.10 -m venv chatbot\n",
        "!chatbot/bin/pip install ipykernel\n",
        "!chatbot/bin/python -m ipykernel install --user --name=chatbot\n",
        "print('Virtual Environment Created!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D5rh05lASA-"
      },
      "source": [
        "### 1.1 Requirements and Environment\n",
        "\n",
        "Requirements (Must be installed before continuing):\n",
        "- Python >= 3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEy02uLUASA-"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSmEdMfJASA_"
      },
      "source": [
        "### 1.2 Importing Required Libraries\n",
        "\n",
        "**Purpose**: Set up the environment, configure logging, suppress warnings, and import dependencies\n",
        "\n",
        "**Input**: None\n",
        "\n",
        "**Output**: Prints \"Dependencies imported successfully.\"\n",
        "\n",
        "**Processing**:\n",
        "- Configures logging with INFO level and standard format.\n",
        "- Disables Hugging Face tokenizers parallelism warnings.\n",
        "- Installs required dependencies silently using pip.\n",
        "- Suppresses non-critical warnings for clean output.\n",
        "- Imports essential libraries for vector store operations and document processing.\n",
        "- Confirms successful imports with a message.\n",
        "\n",
        "**Libraries**:\n",
        "\n",
        "- `os`: For interacting with the operating system and managing environment variables.\n",
        "\n",
        "- `warnings`: For suppressing warning messages during runtime.\n",
        "\n",
        "- `pymilvus.connections` and `pymilvus.utility`: For connecting to and managing the Milvus vector database.\n",
        "\n",
        "- `create_stuff_documents_chain` from `langchain.chains.combine_documents`: For combining documents into a chain.\n",
        "\n",
        "- `Document` from `langchain.schema`: For representing and managing document data structures.\n",
        "\n",
        "- `ChatPromptTemplate` from `langchain_core.prompts`: For creating prompts to structure queries for the language model.\n",
        "\n",
        "- `ChatMistralAI` from `langchain_mistralai.chat_models`: For integrating the Mistral AI language model.\n",
        "\n",
        "- `Milvus` from `langchain_milvus`: For using the Milvus vector database with LangChain.\n",
        "\n",
        "- `RecursiveUrlLoader` from `langchain_community.document_loaders`: For fetching and loading documents recursively from a URL.\n",
        "\n",
        "- `BeautifulSoup` from `bs4`: For parsing HTML and extracting relevant content.\n",
        "\n",
        "- `RecursiveCharacterTextSplitter` from `langchain_text_splitters`: For splitting large text data into smaller chunks for easier processing.\n",
        "\n",
        "- `create_retrieval_chain` from `langchain.chains`: For building chains to retrieve and combine relevant documents.\n",
        "\n",
        "- `HuggingFaceEmbeddings` from `langchain_huggingface`: For generating embeddings using Hugging Face models to encode text into vectors.\n",
        "\n",
        "- `HTTPStatusError` from `httpx`: For handling HTTP status-related exceptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuMMd9ROASA_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Suppress Hugging Face tokenizers parallelism warning\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Installing dependencies if not already installed, suppressing \"Requirement already satisfied\" warnings\n",
        "!pip install -q httpx pymilvus --root-user-action=ignore\n",
        "!pip install -q langchain langchain-core langchain-mistralai langchain-milvus langchain-community beautifulsoup4 langchain-text-splitters langchain-huggingface --root-user-action=ignore\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pymilvus import connections, utility\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.schema import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain_milvus import Milvus\n",
        "from langchain_community.document_loaders import RecursiveUrlLoader\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from httpx import HTTPStatusError\n",
        "\n",
        "print(\"Dependencies imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuOtuMhtASA_"
      },
      "source": [
        "### 1.3 Set Up Environment Variables\n",
        "\n",
        "**Purpose**:\n",
        "\n",
        "To define and load the environment variables required for the RAG (Retrieval-Augmented Generation) operation, including settings for the corpus source, API key, vector store database, and embedding model.\n",
        "\n",
        "**Input**:\n",
        "No direct input to this block, but it implicitly relies on:\n",
        "\n",
        "1. The environment variable `MISTRAL_API_KEY` being set in the system/environment.\n",
        "\n",
        "**Output**:\n",
        "1. Four variables are defined and available for subsequent code execution:\n",
        "\n",
        "- `CORPUS_SOURCE`: URL of the corpus to be processed (default: `'https://www.csusb.edu/its'`).\n",
        "- `MISTRAL_API_KEY`: API key for authenticating with the MistralAI service, fetched from system environment variables.\n",
        "- `MILVUS_URI`: File path for the Milvus Lite vector database (default: `'milvus/jupyter_milvus_vector.db'`).\n",
        "- `MODEL_NAME`: Name of the embedding model for vectorizing documents (default: `'sentence-transformers/all-MiniLM-L12-v2'`).\n",
        "2. Prints \"ENV variables defined.\" to indicate successful setup.\n",
        "\n",
        "**Processing**:\n",
        "1. **Set the Corpus Source**:\n",
        "\n",
        "- Assigns the URL (`https://www.csusb.edu/its`) to the variable `CORPUS_SOURCE`.\n",
        "- This specifies the website from which documents will be loaded for processing.\n",
        "\n",
        "2. **Fetch Mistral API Key**:\n",
        "\n",
        "- Retrieves the API key for MistralAI using `os.environ.get(\"MISTRAL_API_KEY\")`.\n",
        "- This ensures secure access by fetching sensitive credentials from the system's environment variables.\n",
        "3. **Set the Milvus Database Path**:\n",
        "\n",
        "- Defines the file path (`milvus/jupyter_milvus_vector.db`) for the Milvus Lite vector store via `MILVUS_URI`.\n",
        "- This will be used to store and retrieve vector embeddings.\n",
        "4. **Set the Embedding Model Name**:\n",
        "\n",
        "- Specifies the name of the Hugging Face embedding model (`sentence-transformers/all-MiniLM-L12-v2`) in `MODEL_NAME`.\n",
        "- This model is used for transforming text into vector embeddings for document processing.\n",
        "5. **Print Status**:\n",
        "\n",
        "- Prints a confirmation message (`\"ENV variables defined.\"`) to confirm successful setup.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaImJ3wgASBA"
      },
      "outputs": [],
      "source": [
        "CORPUS_SOURCE = 'https://www.csusb.edu/its'\n",
        "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "MILVUS_URI = \"milvus/jupyter_milvus_vector.db\"\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "\n",
        "print(\"ENV variables defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrV052RASBA"
      },
      "source": [
        "## 2.Building the Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrV5R7TrASBA"
      },
      "source": [
        "### 2.1 Vector Store and Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gr_4BPVASBA"
      },
      "source": [
        "#### 2.1.1 Function to Check Vector Store (Milvus database)\n",
        "\n",
        "**Purpose**:\n",
        "To check if the Milvus vector store already exists at the specified URI.\n",
        "\n",
        "**Input**: Path to the Milvus database `uri` (str).\n",
        "\n",
        "**Output**:\n",
        "Returns a boolean (True if the vector store exists, False otherwise).\n",
        "\n",
        "**Processing**:\n",
        "- Creates the `/milvus` directory if it doesn’t exist.\n",
        "- Connects to the Milvus database at the specified uri.\n",
        "- Checks if the collection IT_support exists in the Milvus database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKw6ws8VASBA"
      },
      "outputs": [],
      "source": [
        "def vector_store_check(uri):\n",
        "    \"\"\"\n",
        "    Returns response on whether the vector storage exists\n",
        "\n",
        "    Returns:\n",
        "        boolean\n",
        "    \"\"\"\n",
        "    # Create the directory if it does not exist\n",
        "    head = os.path.split(uri)\n",
        "    os.makedirs(head[0], exist_ok=True)\n",
        "\n",
        "    # Connect to the Milvus database\n",
        "    connections.connect(\"default\", uri=uri)\n",
        "\n",
        "    # Return True if exists, False otherwise\n",
        "    return utility.has_collection(\"IT_support\")\n",
        "\n",
        "print(\"Function `vector_store_check` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqyTX7WsASBA"
      },
      "source": [
        "#### 2.1.2 Function to fetch the embedding model\n",
        "\n",
        "**Purpose**:\n",
        "To load and initialize the embedding model for vectorizing documents.\n",
        "\n",
        "**Input**:\n",
        "None.\n",
        "\n",
        "**Output**: Returns the embedding function loaded from the Hugging Face model specified in `MODEL_NAME`.\n",
        "\n",
        "**Processing**:\n",
        "- Loads the embedding model using `HuggingFaceEmbeddings`.\n",
        "- Returns the initialized embedding function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC9u5DryASBA"
      },
      "outputs": [],
      "source": [
        "def get_embedding_function():\n",
        "    \"\"\"\n",
        "    Returns embedding function for the model\n",
        "\n",
        "    Returns:\n",
        "        embedding function\n",
        "    \"\"\"\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
        "\n",
        "    return embedding_function\n",
        "\n",
        "print(\"Function `get_embedding_function` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl5Jkd_CASBA"
      },
      "source": [
        "### 2.2. Document Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMnXlFYZASBA"
      },
      "source": [
        "#### 2.2.1 Function to Clean Text\n",
        "**Purpose**:\n",
        "To clean a given text by removing extra whitespace and blank lines.\n",
        "\n",
        "**Input**: The input text to be cleaned `text` (str).\n",
        "\n",
        "**Output**:\n",
        "Returns the cleaned text with unnecessary whitespace and blank lines removed.\n",
        "\n",
        "**Processing**:\n",
        "- Splits the text into lines and trims leading/trailing spaces from each line.\n",
        "- Removes empty lines from the text.\n",
        "- Joins the cleaned lines into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C0PpETOASBB"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Further clean the text by removing extra whitespace and new lines.\"\"\"\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    cleaned_lines = [line for line in lines if line]\n",
        "    return '\\n'.join(cleaned_lines)\n",
        "\n",
        "print(\"Function `clean_text` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4OhiuMxASBB"
      },
      "source": [
        "#### 2.2.2 Function to Clean and Extract Text from HTML Content\n",
        "**Purpose**:\n",
        "To extract and clean the main content from an HTML document.\n",
        "\n",
        "**Input**: The HTML content to be cleaned `html_content` (str).\n",
        "\n",
        "**Output**:\n",
        "Returns the cleaned plain text content extracted from the HTML.\n",
        "\n",
        "**Processing**:\n",
        "\n",
        "- Parses the HTML using `BeautifulSoup`.\n",
        "- Removes unnecessary elements like `<script>, <style>, <header>, <footer>, and <nav>`.\n",
        "- Extracts text from the `<main>` tag if it exists, or the entire document otherwise.\n",
        "- Cleans the extracted text using the clean_text function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRqWraDmASBB"
      },
      "outputs": [],
      "source": [
        "def clean_text_from_html(html_content):\n",
        "    \"\"\"Clean HTML content to extract main text.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Remove unnecessary elements\n",
        "    for script_or_style in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
        "        script_or_style.decompose()\n",
        "\n",
        "    main_content = soup.find('main')\n",
        "    if main_content:\n",
        "        content = main_content.get_text(separator='\\n')\n",
        "    else:\n",
        "        content = soup.get_text(separator='\\n')\n",
        "\n",
        "    return clean_text(content)\n",
        "\n",
        "print(\"Function `clean_text_from_html` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki9hACtjASBB"
      },
      "source": [
        "#### 2.2.3 Function for loading documents from the web\n",
        "\n",
        "**Purpose**:\n",
        "To recursively load and clean documents from a web source specified in `CORPUS_SOURCE`.\n",
        "\n",
        "**Input**:\n",
        "None.\n",
        "\n",
        "**Output**:\n",
        "Returns a list of cleaned documents as `Document` objects.\n",
        "\n",
        "**Processing**:\n",
        "\n",
        "- Uses RecursiveUrlLoader to load all pages from the base URL (`CORPUS_SOURCE`).\n",
        "- Iterates through the loaded documents:\n",
        "- Cleans the text using `clean_text_from_html`.\n",
        "- Creates `Document` objects with the cleaned text and metadata.\n",
        "- Returns the list of cleaned `Document` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0YJWcSvASBB"
      },
      "outputs": [],
      "source": [
        "def load_documents_from_web():\n",
        "    \"\"\"\n",
        "    Load the documents from the web and store the page contents\n",
        "\n",
        "    Returns:\n",
        "        list: The documents loaded from the web\n",
        "    \"\"\"\n",
        "    loader = RecursiveUrlLoader(\n",
        "        url=CORPUS_SOURCE,\n",
        "        prevent_outside=True,\n",
        "        base_url=CORPUS_SOURCE\n",
        "        )\n",
        "    raw_documents = loader.load()\n",
        "\n",
        "    # Ensure documents are cleaned\n",
        "    cleaned_documents = []\n",
        "    for doc in raw_documents:\n",
        "        cleaned_text = clean_text_from_html(doc.page_content)\n",
        "        cleaned_documents.append(Document(page_content=cleaned_text, metadata=doc.metadata))\n",
        "\n",
        "    return cleaned_documents\n",
        "\n",
        "print(\"Function `load_documents_from_web` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNyhOMEFASBB"
      },
      "source": [
        "### 2.3 Milvus Vector Store Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qy_IO5IASBB"
      },
      "source": [
        "#### 2.3.1 Function to load existing vector store (Milvus database)\n",
        "\n",
        "**Purpose**: To connect to an existing Milvus vector store.  \n",
        "\n",
        "**Input**: Path to the Milvus database (`uri`).  \n",
        "\n",
        "**Output**: Returns the loaded vector store as a Milvus object.  \n",
        "\n",
        "**Processing**:\n",
        "- Connects to the specified Milvus database.\n",
        "- Initializes a Milvus object with the collection IT_support and the embedding function from `get_embedding_function`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B_P_f4JASBB"
      },
      "outputs": [],
      "source": [
        "def load_existing_db(uri=MILVUS_URI):\n",
        "    \"\"\"\n",
        "    Load an existing vector store from the local Milvus database specified by the URI.\n",
        "\n",
        "    Args:\n",
        "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
        "\n",
        "    Returns:\n",
        "        vector_store: The vector store created\n",
        "    \"\"\"\n",
        "    # Load an existing vector store\n",
        "    vector_store = Milvus(\n",
        "        collection_name=\"IT_support\",\n",
        "        embedding_function=get_embedding_function(),\n",
        "        connection_args={\"uri\": uri},\n",
        "    )\n",
        "\n",
        "    print(\"Vector store loaded\")\n",
        "    return vector_store\n",
        "\n",
        "print(\"Function `load_existing_db` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4dnlGSfASBB"
      },
      "source": [
        "#### 2.3.2 Function to split documents\n",
        "\n",
        "**Purpose**: To split large documents into smaller chunks for better processing and context preservation.  \n",
        "\n",
        "**Input**: List of documents to be split.  \n",
        "\n",
        "**Output**: Returns a list of document chunks.  \n",
        "\n",
        "**Processing**:\n",
        "\n",
        "- Creates a `RecursiveCharacterTextSplitter` with a chunk size of 1000 characters and an overlap of 300 characters.\n",
        "- Splits each document into smaller chunks using the text splitter.\n",
        "- Returns the list of document chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpHoV-ZeASBC"
      },
      "outputs": [],
      "source": [
        "def split_documents(documents):\n",
        "    \"\"\"\n",
        "    Split the documents into chunks\n",
        "\n",
        "    Args:\n",
        "        documents (list): The documents to split\n",
        "\n",
        "    Returns:\n",
        "        list: list of chunks of documents\n",
        "    \"\"\"\n",
        "    # Create a text splitter to split the documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=300,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "\n",
        "    # Split the documents into chunks\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(\"Documents successfully split\")\n",
        "    return docs\n",
        "\n",
        "print(\"Function `split_documents` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2X_1PkVASBC"
      },
      "source": [
        "#### 2.3.3 Function to Create New Vector Store (Milvus database)\n",
        "\n",
        "**Purpose**:\n",
        "To create a new vector store in Milvus using the given documents and embedding function.\n",
        "\n",
        "**Input(s)**:\n",
        "\n",
        "- `docs` (list): Documents to store in the vector database.\n",
        "- `embeddings`: The embedding function for vectorizing documents.\n",
        "- `uri` (str): Path to the Milvus database.  \n",
        "\n",
        "**Output**: Returns the newly created vector store.  \n",
        "\n",
        "**Processing**:\n",
        "\n",
        "- Connects to Milvus at the specified uri.\n",
        "- Creates a new collection named IT_support, dropping any existing one.\n",
        "- Embeds the documents and stores them in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFgsj3gjASBC"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(docs, embeddings, uri):\n",
        "    \"\"\"\n",
        "    This function initializes a vector store using the provided documents and embeddings.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of documents to be stored in the vector store.\n",
        "        embeddings : A function or model that generates embeddings for the documents.\n",
        "        uri (str): Path to the local milvus db\n",
        "\n",
        "    Returns:\n",
        "        vector_store: The vector store created\n",
        "    \"\"\"\n",
        "    # Create a new vector store and drop any existing one\n",
        "    vector_store = Milvus.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"IT_support\",\n",
        "        connection_args={\"uri\": uri},\n",
        "        drop_old=True,\n",
        "    )\n",
        "\n",
        "    print(\"Vector store created\")\n",
        "    return vector_store\n",
        "\n",
        "print(\"Function `create_vector_store` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y6TNg2eASBC"
      },
      "source": [
        "#### 2.3.4 Core function for initializing Milvus\n",
        "\n",
        "**Purpose**: To initialize the Milvus vector store by either loading an existing one or creating a new one.\n",
        "\n",
        "**Input**: Path to the Milvus database `uri`.  \n",
        "\n",
        "**Output**: Returns the initialized vector store.  \n",
        "\n",
        "**Processing**:\n",
        "- Checks if the vector store already exists using vector_store_check.\n",
        "- If it exists:  \n",
        "    - loads it using `load_existing_db`.  \n",
        "- Otherwise:\n",
        "    - Loads documents from the web using `load_documents_from_web`.\n",
        "    - Splits the documents into chunks using `split_documents`.\n",
        "    - Creates a new vector store using `create_vector_store`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMZJ-2slASBC"
      },
      "outputs": [],
      "source": [
        "def initialize_milvus(uri: str=MILVUS_URI):\n",
        "    \"\"\"\n",
        "    Initialize the vector store for the RAG model\n",
        "\n",
        "    Args:\n",
        "        uri (str, optional): Path to the local vector storage. Defaults to MILVUS_URI.\n",
        "\n",
        "    Returns:\n",
        "        vector_store: The vector store created\n",
        "    \"\"\"\n",
        "    if vector_store_check(uri):\n",
        "        vector_store = load_existing_db(uri)\n",
        "        print(\"Embeddings loaded from existing storage\")\n",
        "    else:\n",
        "        embeddings = get_embedding_function()\n",
        "        print(\"Embeddings Loaded\")\n",
        "        documents = load_documents_from_web()\n",
        "        print(\"Documents Loaded\")\n",
        "\n",
        "        # Split the documents into chunks\n",
        "        docs = split_documents(documents=documents)\n",
        "        print(\"Documents Splitting completed\")\n",
        "\n",
        "        vector_store = create_vector_store(docs, embeddings, uri)\n",
        "    print(\"Milvus successfully initialized\")\n",
        "    return vector_store\n",
        "\n",
        "print(\"Function `initialize_milvus` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKaV-fRsASBC"
      },
      "source": [
        "#### 2.3.5 Initializing vector store (Milvus database)\n",
        "**Purpose**: To initialize the Milvus database system for vector storage and retrieval.  \n",
        "\n",
        "**Input**: None.  \n",
        "\n",
        "**Output**: None.  \n",
        "\n",
        "**Processing**: Calls the `initialize_milvus` function to set up the Milvus vector database system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io9WEIxvASBC"
      },
      "outputs": [],
      "source": [
        "print(\"Starting Milvus initialization.\")\n",
        "initialize_milvus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuHcXZCdASBC"
      },
      "source": [
        "## 3. Testing the Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DAXDSsWASBD"
      },
      "source": [
        "### 3.1 Function to create RAG prompt\n",
        "\n",
        "**Purpose**:\n",
        "To create a prompt template for the RAG model with predefined system instructions\n",
        "\n",
        "**Input**: None\n",
        "\n",
        "**Output**: Returns a `ChatPromptTemplate` object\n",
        "\n",
        "**Processing**:\n",
        "- Defines a template with system instructions for generating accurate and context-based responses\n",
        "- Initializes a `ChatPromptTemplate` using the system template and a human prompt structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "levo2gTHASBD"
      },
      "outputs": [],
      "source": [
        "def create_prompt():\n",
        "    \"\"\"\n",
        "    Create a prompt template for the RAG model\n",
        "\n",
        "    Returns:\n",
        "        PromptTemplate: The prompt template for the RAG model\n",
        "    \"\"\"\n",
        "    # Define the prompt template\n",
        "    PROMPT_TEMPLATE = \"\"\"\\\n",
        "    You are an AI assistant that provides answers strictly based on the provided context. Adhere to these guidelines:\n",
        "     - Only answer questions based on the content within the <context> tags.\n",
        "     - If the <context> does not contain information related to the question, respond only with: \"I don't have enough information to answer this question.\"\n",
        "     - For unclear questions or questions that lack specific context, request clarification from the user.\n",
        "     - Provide specific, concise ansewrs. Where relevant information includes statistics or numbers, include them in the response.\n",
        "     - Avoid adding any information, assumption, or external knowledge. Answer accurately within the scope of the given context and do not guess.\n",
        "     - If information is missing, respond only with: \"I don't have enough information to answer this question.\"\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", PROMPT_TEMPLATE),\n",
        "        (\"human\", \"<question>{input}</question>\\n\\n<context>{context}</context>\"),\n",
        "    ])\n",
        "\n",
        "    print(\"Prompt template defined\")\n",
        "    return prompt\n",
        "\n",
        "print(\"Function `create_prompt` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkLn8RvRASBD"
      },
      "source": [
        "### 3.2 Function to query RAG model\n",
        "\n",
        "**Purpose**: To query the RAG model for a response to a user’s question.\n",
        "\n",
        "**Input**: User’s question query.\n",
        "\n",
        "**Output**: Returns the generated response from the RAG model, including source references.\n",
        "\n",
        "**Processing**:\n",
        "\n",
        "- Initializes the MistralAI model.\n",
        "- Loads the prompt template using `create_prompt`.\n",
        "- Loads the vector store and creates a retriever to fetch relevant documents.\n",
        "- Creates a document chain and a retrieval chain for generating responses.\n",
        "- Executes the query and generates a response using the retrieval chain.\n",
        "- Extracts and appends source metadata to the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnLO-7RPASBD"
      },
      "outputs": [],
      "source": [
        "def query_rag(query):\n",
        "    \"\"\"\n",
        "    Entry point for the RAG model to generate an answer to a given query\n",
        "\n",
        "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store,\n",
        "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string for which an answer is to be generated.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer to the query\n",
        "    \"\"\"\n",
        "    # Define the model\n",
        "    model = ChatMistralAI(model='open-mistral-7b')\n",
        "    print(\"Model Loaded\")\n",
        "\n",
        "    prompt = create_prompt()\n",
        "\n",
        "    # Load the vector store and create the retriever\n",
        "    vector_store = load_existing_db(uri=MILVUS_URI)\n",
        "    retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.7, \"k\":5})\n",
        "    try:\n",
        "        document_chain = create_stuff_documents_chain(model, prompt)\n",
        "        print(\"Document Chain Created\")\n",
        "\n",
        "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "        print(\"Retrieval Chain Created\")\n",
        "\n",
        "        # Generate a response to the query\n",
        "        response = retrieval_chain.invoke({\"input\": f\"{query}\"})\n",
        "    except HTTPStatusError as e:\n",
        "        print(f\"HTTPStatusError: {e}\")\n",
        "        if e.response.status_code == 429:\n",
        "            error_message = \"I am currently experiencing high traffic. Please try again later.\"\n",
        "            print(error_message)\n",
        "            return error_message, []\n",
        "        error_message = \"I am unable to answer this question at the moment. Please try again later.\"\n",
        "        print(error_message)\n",
        "        return error_message, []\n",
        "\n",
        "    # logic to add sources to the response\n",
        "    max_relevant_sources = 4 # number of sources at most to be added to the response\n",
        "    all_sources = \"\"\n",
        "    sources = []\n",
        "    count = 1\n",
        "    for i in range(max_relevant_sources):\n",
        "        try:\n",
        "            source = response[\"context\"][i].metadata[\"source\"]\n",
        "            # check if the source is already added to the list\n",
        "            if source not in sources:\n",
        "                sources.append(source)\n",
        "                all_sources += f\"[Source {count}]({source}), \"\n",
        "                count += 1\n",
        "        except IndexError: # if there are no more sources to add\n",
        "            break\n",
        "    all_sources = all_sources[:-2] # remove the last comma and space\n",
        "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
        "    print(\"------------------------------------------------------------------------\")\n",
        "    print(\"Response Generated:\\n\")\n",
        "\n",
        "    return response[\"answer\"]\n",
        "\n",
        "print(\"Function `query_rag` defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S06p6wUnASBD"
      },
      "source": [
        "### 3.3 Get response from RAG\n",
        "\n",
        "**Purpose**: Taking input query from user, executing it and displaying the fetched output.  \n",
        "\n",
        "**Input**: User's question.\n",
        "\n",
        "**Output**: Response generated from the `query_rag` function.\n",
        "\n",
        "**Processing**:\n",
        "- User input: Prompts the user to input a query.\n",
        "- Query execution: Passes the user-provided query to the `query_rag` function.\n",
        "- Response display: Prints the response generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq4aNLCGASBD"
      },
      "outputs": [],
      "source": [
        "response = query_rag(input(\"Enter your query: \"))\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8XhfVgoASBD"
      },
      "source": [
        "## 4. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww_Yv6MoASBD"
      },
      "source": [
        " **Recap**:\n",
        "\n",
        "- Developed an **IT Support chatbot** using the **RAG (Retrieval-Augmented Generation)** system, which retrieves relevant documents and generates responses based on the context provided.\n",
        "- Integrated **widgets** in **Jupyter Notebook** for interactive user input and response display, enabling real-time query processing.\n",
        "- Configured the chatbot with **natural language processing models** like **Mistral AI** and **vector-based document retrieval** using **Milvus**, ensuring accurate and context-aware responses.\n",
        "\n",
        "**Next Steps**:\n",
        "\n",
        "- Expand the chatbot's knowledge base to accommodate a broader range of questions or integrate multiple datasets from diverse sources, enhancing the system’s robustness and versatility.\n",
        "- Improve the chatbot’s performance and extend its capabilities by incorporating more advanced features, such as deeper integrations with external knowledge sources and enhanced response-generation techniques.\n",
        "\n",
        "**Resources**:\n",
        "\n",
        "- \"The **IT Support Chatbot** project was developed by **CSUSB Fall 2024 CSE6550 Team 1**  to provide automated assistance for IT-related queries and technical support.\"\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1)\n",
        "[![Wiki](https://img.shields.io/badge/Wiki-blue?style=flat&logo=wikipedia&logoColor=white)](https://github.com/DrAlzahraniProjects/csusb_fall2024_cse6550_team1/wiki)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}