{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Interactive Mode Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "Importing core libraries: dotenv for environment management, requests and httpx for HTTP requests, pymilvus for vector storage and langchain.Extensions of langchain include core, mistralai, milvus, community, text-splitters and huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Installing dependencies if not already installed\n",
    "!pip install requests httpx pymilvus --root-user-action=ignore\n",
    "!pip install langchain langchain-core langchain-mistralai langchain-cohere langchain-milvus langchain-community langchain-text-splitters langchain-huggingface --root-user-action=ignore\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, utility\n",
    "import sqlite3\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up environment variables\n",
    "\n",
    "\n",
    "Load the necessary environment variables for RAG operation. `CORPUS_SOURCE` can be modified to load a different corpus, `MISTRAL_API_KEY` stores the MistralAI API key, `MILVUS_URI` provides the path for the milvus lite database file, and `MODEL_NAME` specifies the embedding model for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_SOURCE = 'https://www.csusb.edu/its'\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "MILVUS_URI = \"milvus/jupyter_milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"ENV variables defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create Vector Store (Milvus database)\n",
    "\n",
    "Creates the `/milvus` directory if it doesnâ€™t already exist, then attempts to connect to the database file. Returns a boolean indicating whether the database was successfully found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_store_check(uri):\n",
    "    \"\"\"\n",
    "    Returns response on whether the milvus database exists\n",
    "\n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    \n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\",uri=uri)\n",
    "\n",
    "    # Return True if exists, False otherwise\n",
    "    return utility.has_collection(\"IT_support\")\n",
    "\n",
    "print(\"Function `vector_store_check` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fetch the embedding model\n",
    "\n",
    "Loads the embedding model from HuggingFace using the `MODEL_NAME` environment variable and returns the initialized embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "    print(\"Embedding function loaded.\")\n",
    "    return embedding_function\n",
    "\n",
    "print(\"Function `get_embedding_function` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for loading documents from the web\n",
    "\n",
    "Recursively load documents from the web according to `CORPUS_SOURCE`, ensuring that only pages within the base_url of `CORPUS_SOURCE` are retrieved. The function returns the loaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_web():\n",
    "    \"\"\"\n",
    "    Load the documents from the web and store the page contents\n",
    "\n",
    "    Returns:\n",
    "        list: The documents loaded from the web\n",
    "    \"\"\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=CORPUS_SOURCE,\n",
    "        prevent_outside=True,\n",
    "        base_url=CORPUS_SOURCE\n",
    "        )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(\"Documents loaded.\")\n",
    "    return documents\n",
    "\n",
    "print(\"Function `load_documents_from_web` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load existing vector store (Milvus database)\n",
    "\n",
    "Accepts the path to the database and embedding function to establish a connection with the database, returning the connected vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Load an existing vector store\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"IT_support\",\n",
    "        embedding_function = get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    \n",
    "    print(\"Vector store loaded.\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `load_existing_db` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split documents\n",
    "\n",
    "Takes the documents loaded from `load_documents_from_web` and splits them into chunks of 1000 characters. Overlaps 300 characters to preserve context. Returns the documents split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split the documents into chunks\n",
    "\n",
    "    Args:\n",
    "        documents (list): The documents to split\n",
    "\n",
    "    Returns:\n",
    "        list: list of chunks of documents\n",
    "    \"\"\"\n",
    "    # Create a text splitter to split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(\"Documents successfully split.\")\n",
    "    return docs\n",
    "\n",
    "print(\"Function `split_documents` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create vector store (Milvus database)\n",
    "\n",
    "Uses the documents retrieved from `load_documents_from_web`, the embedding function from `get_embedding_function`, and the database path to establish the vector store. It returns the vector store once it has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create a new vector store and drop any existing one\n",
    "    vector_store = Milvus.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"IT_support\",\n",
    "        connection_args={\"uri\": uri},\n",
    "        drop_old=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Vector store created.\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `create_vector_store` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core function for initializing Milvus\n",
    "\n",
    "This is the primary function for initializing Milvus, utilizing the previously defined functions to fully set up the vector store. Executing `initialize_milvus` will invoke all the necessary functions required for creating the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_milvus(uri: str=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Initialize the vector store for the RAG model\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    if vector_store_check(uri):\n",
    "        vector_store = load_existing_db(uri)\n",
    "        print(\"Embeddings loaded from existing Database\")\n",
    "    else:\n",
    "        embeddings = get_embedding_function()\n",
    "        print(\"Embeddings Loaded\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(\"Documents Loaded\")\n",
    "        print(len(documents))\n",
    "    \n",
    "        # Split the documents into chunks\n",
    "        docs = split_documents(documents=documents)\n",
    "        print(\"Documents Splitting completed\")\n",
    "    \n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "\n",
    "    print(\"Milvus successfully initialized.\")\n",
    "    #return vector_store\n",
    "\n",
    "print(\"Function `initialize_milvus` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing vector store (Milvus database)\n",
    "\n",
    "This process may take a considerable amount of time due to the embedding function. Please be patient while it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Milvus initialization.\")\n",
    "initialize_milvus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create RAG prompt\n",
    "\n",
    "Define the `PROMPT_TEMPLATE` and assign roles using `ChatPromptTemplate`. The `<context>` tags represent the document context, while the `<question>` tags represent the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Create a prompt template for the RAG model\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The prompt template for the RAG model\n",
    "    \"\"\"\n",
    "    # Define the prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\\\n",
    "    You are an AI assistant that provides answers strictly based on the provided context. Adhere to these guidelines:\n",
    "     - Only answer questions based on the content within the <context> tags.\n",
    "     - If the <context> does not contain information related to the question, respond only with: \"I don't have enough information to answer this question.\"\n",
    "     - For unclear questions or questions that lack specific context, request clarification from the user.\n",
    "     - Provide specific, concise ansewrs. Where relevant information includes statistics or numbers, include them in the response.\n",
    "     - Avoid adding any information, assumption, or external knowledge. Answer accurately within the scope of the given context and do not guess.\n",
    "     - If information is missing, respond only with: \"I don't have enough information to answer this question.\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", PROMPT_TEMPLATE),\n",
    "        (\"human\", \"<question>{input}</question>\\n\\n<context>{context}</context>\"),\n",
    "    ])\n",
    "\n",
    "    print(\"Prompt template defined.\")\n",
    "    return prompt\n",
    "\n",
    "print(\"Function `create_prompt` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to query RAG model\n",
    "\n",
    "Loads the MistralAI model, the prompt template, and the vector store (Milvus database). Converts the vector store into a retriever that fetches documents containing relevant context. Constructs a document chain that includes all context-related documents. Creates a retrieval chain that utilizes the documents and retriever to gather context based on the user's question. Additionally, retrieves source metadata from the context documents that have been fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatMistralAI(model='open-mistral-7b')\n",
    "    print(\"Model Loaded\")\n",
    "\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)\n",
    "        print(\"Document Chain Created\")\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        print(\"Retrieval Chain Created\")\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain.invoke({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:\n",
    "        print(f\"HTTPStatusError: {e}\")\n",
    "        if e.response.status_code == 429:\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []\n",
    "        return \"I am unable to answer this question at the moment. Please try again later.\", []\n",
    "    \n",
    "    # logic to add sources to the response\n",
    "    max_relevant_sources = 4 # number of sources at most to be added to the response\n",
    "    all_sources = \"\"\n",
    "    sources = []\n",
    "    count = 1\n",
    "    for i in range(max_relevant_sources):\n",
    "        try:\n",
    "            source = response[\"context\"][i].metadata[\"source\"]\n",
    "            # check if the source is already added to the list\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "                all_sources += f\"[Source {count}]({source}), \"\n",
    "                count += 1\n",
    "        except IndexError: # if there are no more sources to add\n",
    "            break\n",
    "    all_sources = all_sources[:-2] # remove the last comma and space\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Response Generated:\\n\")\n",
    "    \n",
    "    return response[\"answer\"]\n",
    "\n",
    "print(\"Function `query_rag` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response from RAG\n",
    "\n",
    "Send a question to RAG, retrieve the response, and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_rag(\"how do I connect to the wifi?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
