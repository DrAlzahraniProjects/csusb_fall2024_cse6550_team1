{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Interactive Mode Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries\n",
    "\n",
    "Import the required libraries needed for the RAG. Core libraries are dotenv, requests, httpx, pymilvus, and langchain. Langchain extensions are core, mistralai, milvus, community, huggingface, and text-splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Installing dependencies if not already installed\n",
    "!pip install os requests httpx pymilvus sqlite3\n",
    "!pip install langchain langchain-core langchain-mistralai langchain-cohere langchain-milvus langchain-community langchain-text-splitters langchain-huggingface\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, utility\n",
    "import sqlite3\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env variables\n",
    "\n",
    "Load the env variables needed for operation of the RAG. `CORPUS_SOURCE` can be changed to load a different corpus. `MISTRAL_API_KEY` contains the MistralAi API key. `MILVUS_URI` is the path for the milvus lite database file. `MODEL_NAME` is the embedding model used on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_SOURCE = 'https://www.csusb.edu/its'\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "MILVUS_URI = \"milvus/jupyter_milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"ENV variables defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create Vector Store (Milvus database)\n",
    "\n",
    "Creates the `/milvus` directory if it doesn't already exist. Attempts to connect to the database file. Return a boolean based on whether the database exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_store_check(uri):\n",
    "    \"\"\"\n",
    "    Returns response on whether the milvus database exists\n",
    "\n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    \n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\",uri=uri)\n",
    "\n",
    "    # Return True if exists, False otherwise\n",
    "    return utility.has_collection(\"IT_support\")\n",
    "\n",
    "print(\"Function `vector_store_check` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to retrieve embedding function\n",
    "\n",
    "Gets to embedding function from HuggingFace based on the `MODEL_NAME` ENV variable. Returns the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "    print(\"Embedding function loaded.\")\n",
    "    return embedding_function\n",
    "\n",
    "print(\"Function `get_embedding_function` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load documents from the web\n",
    "\n",
    "Load documents from the web recursively based on `CORPUS_SOURCE`. Prevents loaded pages that are outside of the base_url of `CORPUS_SOURCE`. Returns the documents that are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_web():\n",
    "    \"\"\"\n",
    "    Load the documents from the web and store the page contents\n",
    "\n",
    "    Returns:\n",
    "        list: The documents loaded from the web\n",
    "    \"\"\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=CORPUS_SOURCE,\n",
    "        prevent_outside=True,\n",
    "        base_url=CORPUS_SOURCE\n",
    "        )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(\"Documents loaded.\")\n",
    "    return documents\n",
    "\n",
    "print(\"Function `load_documents_from_web` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load existing vector store (Milvus database)\n",
    "\n",
    "Takes the path to the database and embedding function and connects to the database. Returns to connected vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Load an existing vector store\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"IT_support\",\n",
    "        embedding_function = get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    \n",
    "    print(\"Vector store loaded.\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `load_existing_db` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split documents\n",
    "\n",
    "Takes the documents loaded from `load_documents_from_web` and splits them into chunks of 1000 characters. Overlaps 300 characters to ensure no context is missing. Returns the documents split into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split the documents into chunks\n",
    "\n",
    "    Args:\n",
    "        documents (list): The documents to split\n",
    "\n",
    "    Returns:\n",
    "        list: list of chunks of documents\n",
    "    \"\"\"\n",
    "    # Create a text splitter to split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(\"Documents successfully split.\")\n",
    "    return docs\n",
    "\n",
    "print(\"Function `split_documents` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create vector store (Milvus database)\n",
    "\n",
    "Takes the documents from `load_documents_from_web`, embedding function from `get_embedding_function`, and database path to create the vector store. Once it is created it returns the created vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create a new vector store and drop any existing one\n",
    "    vector_store = Milvus.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"IT_support\",\n",
    "        connection_args={\"uri\": uri},\n",
    "        drop_old=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Vector store created.\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"Function `create_vector_store` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base function to initialize Milvus\n",
    "\n",
    "Main function in initializing Milvus. Uses the previous functions to fully create the vector store. Running `initialize_milvus` will call all other functions needed to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_milvus(uri: str=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Initialize the vector store for the RAG model\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    if vector_store_check(uri):\n",
    "        vector_store = load_existing_db(uri)\n",
    "        print(\"Embeddings loaded from existing Database\")\n",
    "    else:\n",
    "        embeddings = get_embedding_function()\n",
    "        print(\"Embeddings Loaded\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(\"Documents Loaded\")\n",
    "        print(len(documents))\n",
    "    \n",
    "        # Split the documents into chunks\n",
    "        docs = split_documents(documents=documents)\n",
    "        print(\"Documents Splitting completed\")\n",
    "    \n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "\n",
    "    print(\"Milvus successfully initialized.\")\n",
    "    #return vector_store\n",
    "\n",
    "print(\"Function `initialize_milvus` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize vector store (Milvus database)\n",
    "\n",
    "Due to the embedding function, this will take a significant amount of time. Please be patient until it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Milvus initialization.\")\n",
    "initialize_milvus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create RAG prompt\n",
    "\n",
    "Define the `PROMPT_TEMPLATE` and assign the roles through `ChatPromptTemplate`. `<context>` tags contain the document context. `<question>` tags contain the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Create a prompt template for the RAG model\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The prompt template for the RAG model\n",
    "    \"\"\"\n",
    "    # Define the prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\\\n",
    "    You are an AI assistant that provides answers strictly based on the provided context. Adhere to these guidelines:\n",
    "     - Only answer questions based on the content within the <context> tags.\n",
    "     - If the <context> does not contain information related to the question, respond only with: \"I don't have enough information to answer this question.\"\n",
    "     - For unclear questions or questions that lack specific context, request clarification from the user.\n",
    "     - Provide specific, concise ansewrs. Where relevant information includes statistics or numbers, include them in the response.\n",
    "     - Avoid adding any information, assumption, or external knowledge. Answer accurately within the scope of the given context and do not guess.\n",
    "     - If information is missing, respond only with: \"I don't have enough information to answer this question.\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", PROMPT_TEMPLATE),\n",
    "        (\"human\", \"<question>{input}</question>\\n\\n<context>{context}</context>\"),\n",
    "    ])\n",
    "\n",
    "    print(\"Prompt template defined.\")\n",
    "    return prompt\n",
    "\n",
    "print(\"Function `create_prompt` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to query RAG model\n",
    "\n",
    "Loads the MistralAI model, prompt template, and vector store (Milvus database). Converts the vector store into a retriever, which will retrieve the documents containing context. Create a document chain containing all the documents with context. Create a retrieval_chain that uses the documents and retriever to load context based on user question. Based on the retrieved context documents, retrieve source metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatMistralAI(model='open-mistral-7b')\n",
    "    print(\"Model Loaded\")\n",
    "\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)\n",
    "        print(\"Document Chain Created\")\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        print(\"Retrieval Chain Created\")\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain.invoke({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:\n",
    "        print(f\"HTTPStatusError: {e}\")\n",
    "        if e.response.status_code == 429:\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []\n",
    "        return \"I am unable to answer this question at the moment. Please try again later.\", []\n",
    "    \n",
    "    # logic to add sources to the response\n",
    "    max_relevant_sources = 4 # number of sources at most to be added to the response\n",
    "    all_sources = \"\"\n",
    "    sources = []\n",
    "    count = 1\n",
    "    for i in range(max_relevant_sources):\n",
    "        try:\n",
    "            source = response[\"context\"][i].metadata[\"source\"]\n",
    "            # check if the source is already added to the list\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "                all_sources += f\"[Source {count}]({source}), \"\n",
    "                count += 1\n",
    "        except IndexError: # if there are no more sources to add\n",
    "            break\n",
    "    all_sources = all_sources[:-2] # remove the last comma and space\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Response Generated:\\n\")\n",
    "    \n",
    "    return response[\"answer\"]\n",
    "\n",
    "print(\"Function `query_rag` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get response from RAG\n",
    "\n",
    "Send RAG a user question and get the RAG response. Print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_rag(\"how do I connect to the wifi?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
